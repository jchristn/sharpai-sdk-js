// OpenAI Completion Response
export const mockCompletionResponse = {
  id: 'cmpl-656',
  object: 'text_completion',
  created: 1763710823,
  model: 'llama3.1:latest',
  system_fingerprint: 'fp_ollama',
  choices: [
    {
      text: "...there existed a magnificent planet called Aethoria, where the skies were painted with colors that would put even the most skilled artist to shame. The air was sweet with the scent of blooming starflowers, and the land was alive with ancient magic that whispered secrets to those who listened closely.\n\nOn this enchanted world, there lived a young space explorer named Lyra, whose heart was filled with wonder and curiosity about the mysteries of the cosmos. With her trusty ship, the Celestial Dreamer, she had spent years charting the unexplored reaches of Aethoria's vast wilderness, discovering hidden wonders and unraveling the secrets of the planet's ancient past.\n\nOne day, as Lyra navigated through a swirling vortex of iridescent clouds, her ship was intercepted by an unknown vessel. The airlocks opened with a hiss, and a figure clad in shimmering silver robes stepped aboard...",
      index: 0,
      finish_reason: 'stop',
    },
  ],
  usage: {
    prompt_tokens: 20,
    completion_tokens: 183,
    total_tokens: 203,
  },
};

// OpenAI Chat Completion Response
export const mockChatCompletionResponse = {
  id: 'chatcmpl-123',
  object: 'chat.completion',
  created: 1763710823,
  model: 'llama3.1:latest',
  choices: [
    {
      index: 0,
      message: {
        role: 'assistant',
        content: 'Hello! How can I assist you today?',
      },
      finish_reason: 'stop',
    },
  ],
  usage: {
    prompt_tokens: 10,
    completion_tokens: 8,
    total_tokens: 18,
  },
};

// OpenAI Embeddings Response
export const mockEmbeddingsResponse = {
  object: 'list',
  data: [
    {
      object: 'embedding',
      embedding: [0.1, 0.2, 0.3, 0.4, 0.5],
      index: 0,
    },
  ],
  model: 'llama3.1:latest',
  usage: {
    prompt_tokens: 5,
    total_tokens: 5,
  },
};

// Ollama Generate Completion Response
export const mockOllamaCompletionResponse = {
  model: 'llama3.1:latest',
  created_at: '2024-01-01T00:00:00.000Z',
  response: 'This is a test completion response from Ollama.',
  done: true,
  done_reason: 'stop',
  context: [1, 2, 3, 4, 5],
  total_duration: 1000,
  load_duration: 100,
  prompt_eval_count: 10,
  prompt_eval_duration: 200,
  eval_count: 20,
  eval_duration: 700,
};

// Ollama Chat Completion Response
export const mockOllamaChatCompletionResponse = {
  model: 'llama3.1:latest',
  created_at: '2024-01-01T00:00:00.000Z',
  message: {
    role: 'assistant',
    content: 'Hello from Ollama chat!',
  },
  done_reason: 'stop',
  done: true,
  total_duration: 1000,
  load_duration: 100,
  prompt_eval_count: 10,
  prompt_eval_duration: 200,
  eval_count: 20,
  eval_duration: 700,
};

// Ollama Model List Response
export const mockModelListResponse = {
  models: [
    {
      name: 'llama3.1:latest',
      model: 'llama3.1',
      modified_at: '2024-01-01T00:00:00.000Z',
      size: 4096000000,
      digest: 'sha256:abc123',
      details: {
        parent_model: '',
        format: 'gguf',
        family: 'llama',
        families: ['llama'],
        parameter_size: '8B',
        quantization_level: 'Q4_0',
      },
    },
    {
      name: 'mistral:latest',
      model: 'mistral',
      modified_at: '2024-01-01T00:00:00.000Z',
      size: 4096000000,
      digest: 'sha256:def456',
      details: {
        parent_model: '',
        format: 'gguf',
        family: 'mistral',
        families: ['mistral'],
        parameter_size: '7B',
        quantization_level: 'Q4_0',
      },
    },
  ],
};

// Ollama Model Information Response
export const mockModelInformationResponse = {
  modelfile: '# Modelfile generated by "ollama show"\nFROM llama3.1:latest\n',
  parameters: 'num_ctx 4096',
  template: '{{ .Prompt }}',
  details: {
    parent_model: '',
    format: 'gguf',
    family: 'llama',
    families: ['llama'],
    parameter_size: '8B',
    quantization_level: 'Q4_0',
  },
};

// Ollama Pull Model Response
export const mockPullModelResponse = {
  status: 'pulling manifest',
};

// Ollama Embeddings Response
export const mockOllamaEmbeddingsResponse = {
  embedding: [0.1, 0.2, 0.3, 0.4, 0.5],
};

// Frontend Response
export const mockFrontend: any = {
  Identifier: 'frontend-1',
  Name: 'Test Frontend',
  Hostname: 'example.com',
  TimeoutMs: 30000,
  LoadBalancing: 'round-robin',
  BlockHttp10: false,
  MaxRequestBodySize: 1048576,
  Backends: ['backend-1'],
  RequiredModels: [],
  LogRequestFull: false,
  LogRequestBody: false,
  LogResponseBody: false,
  UseStickySessions: false,
  StickySessionExpirationMs: 3600000,
  PinnedEmbeddingsProperties: null,
  PinnedCompletionsProperties: null,
  AllowEmbeddings: true,
  AllowCompletions: true,
  AllowRetries: true,
  Active: true,
  CreatedUtc: '2024-01-01T00:00:00.000Z',
  LastUpdateUtc: '2024-01-01T00:00:00.000Z',
};

// Frontend List Response
export const mockFrontendListResponse = [mockFrontend];

// Backend Response
export const mockBackend: any = {
  Identifier: 'backend-1',
  Name: 'Test Backend',
  Hostname: 'localhost',
  Port: 11434,
  Ssl: false,
  UnhealthyThreshold: 3,
  HealthyThreshold: 2,
  HealthCheckMethod: 'GET',
  HealthCheckUrl: '/api/tags',
  MaxParallelRequests: 10,
  RateLimitRequestsThreshold: 100,
  LogRequestFull: false,
  LogRequestBody: false,
  LogResponseBody: false,
  ApiFormat: 'ollama',
  Labels: [],
  PinnedEmbeddingsProperties: null,
  PinnedCompletionsProperties: null,
  AllowEmbeddings: true,
  AllowCompletions: true,
  Active: true,
  CreatedUtc: '2024-01-01T00:00:00.000Z',
  LastUpdateUtc: '2024-01-01T00:00:00.000Z',
  ActiveRequests: 0,
  IsSticky: false,
};

// Backend List Response
export const mockBackendListResponse = [mockBackend];

// Backend Health Response
export const mockBackendHealth: any = {
  ...mockBackend,
  UnhealthySinceUtc: null,
  Downtime: '0s',
};

// Backend Health List Response
export const mockBackendHealthListResponse = [mockBackendHealth];
